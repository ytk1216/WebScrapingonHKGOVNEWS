{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape press releases of HKGOV from Jan to June, 2020.\n",
    "\n",
    "Tools: Seleium, BeautifullSoup, Regex, Pandas, Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library we need\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Chrome Options\n",
    "chrome_options = Options() \n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "\n",
    "# define Chrome webdriver\n",
    "driver = webdriver.Chrome(r'C:\\Users\\xxx',options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a root link of the website & other details for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample link for reference ==>  https://www.info.gov.hk/gia/general/202001/09/P2020010900907.htm\n",
    "\n",
    "URL Analysis\n",
    "\n",
    "root link --> https://www.info.gov.hk/gia/general/\n",
    "\n",
    "month     --> 202001/\n",
    "\n",
    "date      --> 09/\n",
    "\n",
    "press ID  --> P2020010900907"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is very simple, we get press IDs of everyday first. Then we scape contents of each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.info.gov.hk/'\n",
    "datedict = {'1':31, '2':29, '3':31, '4':30, '5':31, '6':30}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.info.gov.hk/gia/general/202005/30/P2020053000753.htm 1 times try\n"
     ]
    }
   ],
   "source": [
    "# set up a empty list for data storage\n",
    "l=[]\n",
    "\n",
    "\n",
    "for i in datedict:\n",
    "    for j in range(1,datedict[i]+1):\n",
    "        if j in range(1,10):\n",
    "            j = '0'+str(j)\n",
    "        url = URL +'gia/general/20200'+i+'/'+str(j)+'.htm'\n",
    "        driver.get(url)\n",
    "        waitxpath = '//*[@id=\"contentBody\"]/div[1]/div[1]/ul/li[1]/a'\n",
    "        try:\n",
    "            element_present = EC.presence_of_element_located((By.XPATH, waitxpath))\n",
    "            WebDriverWait(driver, 10).until(element_present)\n",
    "        except:\n",
    "            print(driver.current_url, 'die')\n",
    "            break\n",
    "        date_url_list=bs(requests.get(driver.current_url).text, 'html.parser').find_all(class_='NEW')\n",
    "        for k in date_url_list:\n",
    "            d={}\n",
    "            new_url = URL[:-1]+k['href']\n",
    "            soup = bs(requests.get(new_url).text, 'html.parser')\n",
    "            \n",
    "            trial = 1\n",
    "            while not soup.find(id=\"PRHeadlineSpan\"):\n",
    "                soup = bs(requests.get(new_url).text, 'html.parser')\n",
    "                print(new_url, trial ,'times try')\n",
    "                trial += 1\n",
    "                \n",
    "            d['Date']='2020-'+i+'-'+str(j)\n",
    "            try:\n",
    "                d['time']=re.search(r'HKT \\d\\d:\\d\\d', soup.find(id=\"pressrelease\").text).group()\n",
    "            except:\n",
    "                d['time']=np.nan\n",
    "            d['Title']=soup.find(id=\"PRHeadlineSpan\").text\n",
    "            d['Content']=soup.find(id=\"pressrelease\").text.replace('\\n','').replace('\\t','').replace('\\xa0','').replace('Ã¢\\x80\\x8b','').replace('\\r','')\n",
    "            d['url']=new_url\n",
    "            l.append(d)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(l)\n",
    "df.to_csv('gov_2020_jan-jun_news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of rows in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4788"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
